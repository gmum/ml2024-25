{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Google Colab compatible version\n",
    "The following setup is suitable for Google Colab with T4 GPU.\n",
    "\n",
    "To enable T4 GPU: Connect configuration menu $\\rightarrow$ Change runtime type $\\rightarrow$ Hardware accelerator: T4 GPU"
   ],
   "id": "a1e18eb1ffb8de4f"
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/gmum/ml2024-25.git"
   ],
   "metadata": {
    "id": "SWqMy6ZaNaNh",
    "outputId": "99fd028a-893a-4e33-dbf9-973ba9dce0af",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "SWqMy6ZaNaNh",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('/content/ml2024-25')  # This ensures import lab.checker later"
   ],
   "metadata": {
    "id": "MYTnYPXiNfeX"
   },
   "id": "MYTnYPXiNfeX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121",
   "metadata": {
    "id": "VSofaWydPmPQ"
   },
   "id": "VSofaWydPmPQ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu121/repo.html"
   ],
   "metadata": {
    "id": "l0xwa3iVNgq6",
    "outputId": "a13191e1-6ab2-4664-bee5-8ce0acb20514",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "l0xwa3iVNgq6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install dgllife"
   ],
   "metadata": {
    "id": "HxqjqYJqNhja",
    "outputId": "b6078684-bad3-45eb-8bd1-f488c53fd956",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "HxqjqYJqNhja",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch_geometric\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu121.html  # Optional dependencies"
   ],
   "metadata": {
    "id": "-O2Cv_eRNiUg",
    "outputId": "013150ed-87ac-40f6-a7fa-a3ead17b7701",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "-O2Cv_eRNiUg",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rdkit==2022.3.5"
   ],
   "metadata": {
    "id": "6laojKTbNjDQ",
    "outputId": "95f69dde-976f-423e-f7e3-c6139eecc74c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "6laojKTbNjDQ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchmetrics"
   ],
   "metadata": {
    "id": "QFQ6vZvwNj21",
    "outputId": "40d07779-3cb9-4b73-d1c5-c7b4ea166d87",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "QFQ6vZvwNj21",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Neural Networks (GNNs)\n",
    "## Part II: Transformer\n",
    "We are going to play with attention in the context for molecular property prediciton. It's recommended that you're familiar with the recent lectures on GNNs and with the previous lab on MPNN in molecular property prediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5348b57746426752"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "We are going to use the same dataset and most of the codebase as in the previous lab."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ddf75df96204316"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copied code\n",
    "I'm going to copy most of the code for convenience."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79e432bdca0c548d"
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "from typing import Type\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "from dgl.data import Subset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgllife.data import FreeSolv\n",
    "from dgllife.utils import CanonicalAtomFeaturizer, SMILESToBigraph\n",
    "from dgllife.utils import ScaffoldSplitter\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torchmetrics import Metric\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from lab.checker import expected_gat_output, expected_dot_attention_output, sub_optimal_multihead_attention_output, \\\n",
    "    expected_multihead_attention_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c423333bf850de1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class LoggerBase(ABC):\n",
    "    def __init__(self, logdir: str | Path):\n",
    "        self.logdir = Path(logdir)\n",
    "        self.logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class WandbLogger(LoggerBase):\n",
    "    def __init__(\n",
    "            self, logdir: str | Path, project_name: str, experiment_name: str, **kwargs: Dict[str, Any]\n",
    "    ):\n",
    "        super().__init__(logdir)\n",
    "        import wandb\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.kwargs = kwargs\n",
    "        self.run = wandb.init(\n",
    "            dir=self.logdir,\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        metrics = {f\"{prefix}/{k}\": v for k, v in metrics.items()}\n",
    "        self.run.log(metrics)\n",
    "\n",
    "    def close(self):\n",
    "        self.run.finish()\n",
    "\n",
    "\n",
    "class DummyLogger(LoggerBase):  # If you don't want to use any logger, you can use this one\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def restart(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MetricList:\n",
    "    def __init__(self, metrics: Dict[str, Metric]):\n",
    "        self.metrics = copy.deepcopy(metrics)\n",
    "\n",
    "    def update(self, preds: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric.update(preds.detach().cpu(), targets.cpu())\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        for name, metric_fn in self.metrics.items():\n",
    "            metrics[name] = metric_fn.compute().item()\n",
    "            metric_fn.reset()\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            run_dir: str | Path,\n",
    "            train_dataset: Subset,\n",
    "            valid_dataset: Subset,\n",
    "            train_metrics: Dict[str, Metric],\n",
    "            valid_metrics: Dict[str, Metric],\n",
    "            model: nn.Module,\n",
    "            logger: LoggerBase,\n",
    "            optimizer_kwargs: Dict[str, Any],\n",
    "            optimizer_cls: Type[torch.optim.Optimizer] = torch.optim.Adam,\n",
    "            n_epochs: int,\n",
    "            train_batch_size: int = 32,\n",
    "            valid_batch_size: int = 16,\n",
    "            device: str = \"cuda\",\n",
    "            valid_every_n_epochs: int = 1,\n",
    "            loss_fn=nn.MSELoss()\n",
    "    ):\n",
    "        self.run_dir = Path(run_dir)\n",
    "        self.train_loader = GraphDataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        self.valid_loader = GraphDataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=valid_batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        self.train_metrics = MetricList(train_metrics)\n",
    "        self.valid_metrics = MetricList(valid_metrics)\n",
    "        self.logger = logger\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer_cls(model.parameters(), **optimizer_kwargs)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        self.valid_every_n_epochs = valid_every_n_epochs\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, dataloader: GraphDataLoader, prefix: str) -> Dict[str, float]:\n",
    "        previous_mode = self.model.training\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        for _, graphs, labels in dataloader:\n",
    "            graphs = graphs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            preds = self.model(graphs)\n",
    "            loss = self.loss_fn(preds, labels)\n",
    "            losses.append(loss.item())\n",
    "            self.valid_metrics.update(preds, labels)\n",
    "        self.model.train(mode=previous_mode)\n",
    "        metrics = {\"loss\": np.mean(losses)} | self.valid_metrics.compute()\n",
    "        self.logger.log_metrics(metrics=metrics, prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "    def train(self) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        valid_metrics = {}\n",
    "        for epoch in tqdm(range(self.n_epochs), total=self.n_epochs):\n",
    "            for _, graphs, labels in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                graphs = graphs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                preds = self.model(graphs)\n",
    "                loss = self.loss_fn(preds, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.train_metrics.update(preds, labels)\n",
    "                train_metrics = {\"loss\": loss.item()} | self.train_metrics.compute()\n",
    "                self.logger.log_metrics(metrics=train_metrics, prefix=\"train\")\n",
    "\n",
    "                if epoch % self.valid_every_n_epochs == 0 or epoch == self.n_epochs - 1:\n",
    "                    valid_metrics = self.validate(self.valid_loader, prefix=\"valid\")\n",
    "\n",
    "        return valid_metrics\n",
    "\n",
    "    def test(self, dataset: Subset) -> Dict[str, float]:\n",
    "        dataloader = GraphDataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return self.validate(dataloader, prefix=\"test\")\n",
    "\n",
    "    def close(self):  # close the logger, not really required for wandb\n",
    "        self.logger.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1609cbd2996b027",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def to_dense_embeddings(node_embeddings: torch.Tensor, graph: dgl.DGLGraph, fill_value: float = 0.0) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Converts sparse node embeddings to dense node embeddings with padding.\n",
    "    Arguments:\n",
    "        node_embeddings: node embeddings in a sparse format, i.e. [total_num_nodes, hidden_size]\n",
    "        graph: a batch of graphs\n",
    "        fill_value: a value to fill the padding with\n",
    "    Returns:\n",
    "        node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        mask: a mask indicating which nodes are real and which are padding, i.e. [batch_size, max_num_nodes]\n",
    "    \"\"\"\n",
    "    num_nodes = graph.batch_num_nodes()  # e.g. [2, 3, 3]\n",
    "    indices = torch.arange(len(num_nodes), device=num_nodes.device)\n",
    "    batch = torch.repeat_interleave(indices, num_nodes).long()  # e.g. [0, 0, 1, 1, 1, 2, 2, 2]\n",
    "    return to_dense_batch(node_embeddings, batch,\n",
    "                          fill_value=fill_value)  # that's the only reason we have torch_geometric in the requirements\n",
    "\n",
    "\n",
    "def to_sparse_embeddings(node_embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts dense node embeddings to sparse node embeddings.\n",
    "    Arguments:\n",
    "        node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        mask: a mask indicating which nodes are real and which are padding, i.e. [batch_size, max_num_nodes]\n",
    "    Returns:\n",
    "        node_embeddings: node embeddings in a sparse format, i.e. [total_num_nodes, hidden_size]\n",
    "    \"\"\"\n",
    "    return node_embeddings[mask]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8808a77d7c57aed1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class ReadoutBase(nn.Module, ABC):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class SumReadout(ReadoutBase):\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            node_embeddings: node embeddings in a sparse format, i.e. [total_num_nodes, hidden_size]\n",
    "            graph: a DGLGraph that contains the graph structure\n",
    "        Returns:\n",
    "            graph_embeddings: graph embeddings of shape.[batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # We can also use dgl.sum_nodes function, but let assume it's forbidden in that notebook ;)\n",
    "        node_embeddings, _ = to_dense_embeddings(node_embeddings, graph)\n",
    "        return node_embeddings.sum(dim=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6071a391cab1d058",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updated code\n",
    "Some of the previous code is slightly updated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fc0e088c6c18d87"
  },
  {
   "cell_type": "code",
   "source": [
    "class GNNLayerBase(ABC, nn.Module):\n",
    "    def _init(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor | None,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_features_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 gnn_layer_cls: Type[GNNLayerBase],\n",
    "                 gnn_layer_kwargs: Dict[str, Any],\n",
    "                 gnn_n_layers: int,\n",
    "                 gnn_layer_requires_dense: bool,\n",
    "                 readout_cls: Type[ReadoutBase]):\n",
    "        super().__init__()\n",
    "        self.linear_node = nn.Linear(node_features_size, hidden_size)\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            gnn_layer_cls(hidden_size=hidden_size, **gnn_layer_kwargs)\n",
    "            for _ in range(gnn_n_layers)\n",
    "        ])\n",
    "        self.readout = readout_cls(hidden_size=hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.gnn_layer_requires_dense = gnn_layer_requires_dense\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        node_embeddings = graph.ndata['h']\n",
    "        node_embeddings = self.linear_node(node_embeddings)\n",
    "        if self.gnn_layer_requires_dense:\n",
    "            node_embeddings, mask = to_dense_embeddings(node_embeddings, graph)\n",
    "        else:\n",
    "            mask = None\n",
    "        for layer in self.gnn_layers:\n",
    "            node_embeddings = layer(node_embeddings=node_embeddings, mask=mask, graph=graph)\n",
    "        if self.gnn_layer_requires_dense:\n",
    "            node_embeddings = to_sparse_embeddings(node_embeddings, mask)\n",
    "        graph_embedding = self.readout(node_embeddings, graph)\n",
    "        predictions = self.mlp(graph_embedding)\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482754c146bd3073",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def test_gnn_layer(mpnn_layer_cls: Type[GNNLayerBase], expected_output: torch.Tensor, requires_dense: bool = False):\n",
    "    node_featurizer = CanonicalAtomFeaturizer()\n",
    "    graph_a = SMILESToBigraph(node_featurizer=node_featurizer, add_self_loop=True)(\"CN(C)C(=O)c1ccc(cc1)OC\")\n",
    "    graph_b = SMILESToBigraph(node_featurizer=node_featurizer, add_self_loop=True)(\"CS(=O)(=O)Cl\")\n",
    "    torch.manual_seed(0)\n",
    "    graph = dgl.batch([graph_a, graph_b])\n",
    "    linear_nodes = nn.Linear(node_featurizer.feat_size(), 12)\n",
    "    node_embeddings = linear_nodes(graph.ndata['h'])\n",
    "    layer = mpnn_layer_cls(hidden_size=12)\n",
    "    if requires_dense:\n",
    "        node_embeddings, mask = to_dense_embeddings(node_embeddings, graph)\n",
    "    else:\n",
    "        mask = None\n",
    "    result = layer(node_embeddings=node_embeddings, mask=mask, graph=graph)\n",
    "    assert torch.allclose(result, expected_output, atol=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d82b677d7d9db4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention\n",
    "As you know from the lectures, the Transformer architecture is based on the attention mechanism. However, it's worth noting that attention is not a mechanism from the Transformer architecture, but rather a general concept used many times before. We've actually used some form of attention in the previous lab in `AttentionReadout` class. It was aggregating the information from graph nodes by computing a weighted average of the nodes embeddings using dynamically computed weights. The attention in the Transformer architecture is very similar - the main difference is that attention in Transformer is computed for every token (node) and is used to update the token/node embedding. In the context of graphs, we have a second well-known attention-based architecture called [Graph Attention Network (GAT)](https://arxiv.org/abs/1710.10903). The attention in GAT differs from attention in Transformer in two ways:\n",
    "1. In GAT, the attention for a given node is computed using the node's neighbors while in Transformer it's computed using all other nodes.\n",
    "2. GAT uses additive attention, while Transformer uses (multiheaded) dot-product one.\n",
    "\n",
    "Let's start with implementation of GAT layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f029737734a971b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additive attention (GAT)\n",
    "### Task 1. GATLayer (2 points).\n",
    "Your task is to:\n",
    "1\\) implement a GAT layer which is MPNN that uses attention mechanism for neighbor aggregation. The formula for the update is the following:\n",
    "$$\n",
    "m_i = W_1 x_i\n",
    "$$\n",
    "$$\n",
    "score_{ij} = LeakyReLU(W_2(m_i | m_j)),\n",
    "$$\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{exp(score_{ij})}{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{i\\}} exp(score_{ik})}\n",
    "$$\n",
    "$$\n",
    "x'_i = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij} m_j\n",
    "$$\n",
    "where | stands for concatenation. Note that in our code, we added self-loops to the graph, so $\\mathcal{N}(i) \\cup \\{i\\} = \\mathcal{N}(i)$.\n",
    "\n",
    "2\\) Briefly explain why the attention in GAT is called additive:\n",
    "\n",
    "Your explanation: ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0a9a8af402d16ee"
  },
  {
   "cell_type": "code",
   "source": [
    "class GATLayer(GNNLayerBase):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor | None,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        start_nodes, end_nodes = graph.edges(\n",
    "            order='srcdst')  # note that our graph is bi-directed and contains self-loops\n",
    "        messages = self.linear_1(node_embeddings)\n",
    "        messages_i = messages[start_nodes]\n",
    "        messages_j = messages[end_nodes]\n",
    "\n",
    "        ...\n",
    "\n",
    "\n",
    "test_gnn_layer(GATLayer, expected_gat_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de5dd48a3fc6580c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dot-product attention\n",
    "Having the GAT implemented, we can move to the attention used in the Transformer model. It's called \"multi-head dot-product attention\", but for the moment let's focus on the \"dot-product\" part of the name."
   ],
   "metadata": {
    "collapsed": false,
    "id": "7b21c9263771dfe"
   },
   "id": "7b21c9263771dfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2. Dot-product attention (2 points).\n",
    "Your task is to:\n",
    "1\\) implement the dot-product attention given by the following formula:\n",
    "$$\n",
    "v_i = W_v x_i, \\quad k_i = W_k x_i, \\quad q_i = W_q x_i, \\quad W_v, W_k, W_q \\in \\mathbb{R}^{D \\times D}, \\quad x_i \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "$$\n",
    "score_{ij} = \\frac{q_i^T k_j}{\\sqrt{D}}\n",
    "$$\n",
    "$$\n",
    "a_{ij} = \\frac{exp(score_{ij})}{\\sum_{k=1}^n exp(score_{ik})}\n",
    "$$\n",
    "$$\n",
    "x'_i = \\sum_{j=1}^n a_{ij} v_j\n",
    "$$\n",
    "2\\) Briefly explain what's the advantage of dot-product attention over the additive one.\n",
    "Your explanation: ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfb71775af4b9db9"
  },
  {
   "cell_type": "code",
   "source": [
    "class DotProductAttention(GNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, output_size: int | None = None):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size or hidden_size\n",
    "        self.linear_v = nn.Linear(hidden_size, self.output_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, self.output_size)\n",
    "        self.linear_q = nn.Linear(hidden_size, self.output_size)\n",
    "        self.sqrt_d = np.sqrt(self.output_size)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "            mask: a mask indicating which nodes are real, i.e. [batch_size, max_num_nodes]\n",
    "        Returns:\n",
    "            node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        \"\"\"\n",
    "        values = self.linear_v(node_embeddings)\n",
    "        keys = self.linear_k(node_embeddings)\n",
    "        queries = self.linear_q(node_embeddings)\n",
    "\n",
    "        ...\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=6)\n",
    "test_gnn_layer(DotProductAttention, expected_dot_attention_output, requires_dense=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f526e48a689c5e0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-head dot-product attention\n",
    "Great! We can now implement the actual attention used in the Transformer!\n",
    "### Task 3. Multi-head dot-product attention (2 points).\n",
    "Your task is to:\n",
    "1\\) Implement multi-head dot-product attention (in a naive form). Let's parametrize the attention from the previous task with $W_v, W_k, W_q$ matrices so that we can rewrite it:\n",
    "$$\n",
    "x'_i = attention(W_v, W_k, W_q)\n",
    "$$\n",
    "The multi-head attention is a simple concatenation of the outputs of different attentions and is give by the following formula:\n",
    "$$\n",
    "x'_i = concat(attention_1(W_v^{(1)}, W_k^{(1)}, W_q^{(1)}), \\dots, attention_H(W_v^{(H)}, W_k^{(H)}, W_q^{(H)}))W\n",
    "$$\n",
    "where matrices $W_v^{(h)}, W_k^{(h)}, W_q^{(h)} \\in \\mathbb{R}^{(D / H) \\times D}$ are distinct for each head $h \\in \\{1, \\dots, H\\}$ and have reduced output sizes. The matrix $W \\in \\mathbb{R}^{D \\times D}$.\n",
    "2\\) Briefly explain what is the advantage of the multi-head dot-product attention over the (single-head) dot-product one.\n",
    "Your explanation: ...\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d343acfdedd08d0e"
  },
  {
   "cell_type": "code",
   "source": [
    "class SuboptimalMultiHeadAttention(GNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, n_heads: int = 3):\n",
    "        super().__init__()\n",
    "        assert hidden_size % n_heads == 0\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size)  # the last linear layer (W)\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for _ in range(n_heads):\n",
    "            attention = DotProductAttention(\n",
    "                ...\n",
    "            )\n",
    "            self.attentions.append(attention)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "test_gnn_layer(SuboptimalMultiHeadAttention, sub_optimal_multihead_attention_output, requires_dense=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4630245780b0520",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4. Multi-head dot-product attention (2 points).\n",
    "The attention from the previous task works, but requires to loop over `n_attention` heads. We can avoid that! Your task is to implement multi-head dot-product attention defined in the previous task in a batch-manner (without using loops)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9729ec99e9d044ea"
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(GNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, n_heads: int = 3):\n",
    "        super().__init__()\n",
    "        assert hidden_size % n_heads == 0\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_heads = n_heads\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.sqrt_d = np.sqrt(hidden_size // n_heads)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "            mask: a mask indicating which nodes are real, i.e. [batch_size, max_num_nodes]\n",
    "        Returns:\n",
    "            node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        \"\"\"\n",
    "        values = self.linear_v(node_embeddings)\n",
    "        keys = self.linear_k(node_embeddings)\n",
    "        queries = self.linear_q(node_embeddings)\n",
    "\n",
    "        values = values.reshape(values.shape[0], values.shape[1], self.n_heads,\n",
    "                                -1)  # [batch_size, max_num_nodes, n_heads, hidden_size // n_heads]\n",
    "        keys = keys.reshape(keys.shape[0], keys.shape[1], self.n_heads,\n",
    "                            -1)  # [batch_size, max_num_nodes, n_heads, hidden_size // n_heads]\n",
    "        queries = queries.reshape(queries.shape[0], queries.shape[1], self.n_heads,\n",
    "                                  -1)  # [batch_size, max_num_nodes, n_heads, hidden_size // n_heads]\n",
    "        values = values.transpose(-2, -3)  # [batch_size, n_heads, max_num_nodes, hidden_size // n_heads]\n",
    "        keys = keys.transpose(-2, -3)  # [batch_size, n_heads, max_num_nodes, hidden_size // n_heads]\n",
    "        queries = queries.transpose(-2, -3)  # [batch_size, n_heads, max_num_nodes, hidden_size // n_heads]\n",
    "\n",
    "        ...\n",
    "\n",
    "\n",
    "test_gnn_layer(MultiHeadAttention, expected_multihead_attention_output, requires_dense=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba21c935ca28de42",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "We have implemented the attention mechanism. Transformer also consist of other parts but they're rather boring and here's the implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1831a36671fc22ad"
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerLayer(GNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, n_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = MultiHeadAttention(hidden_size=hidden_size, n_heads=n_heads)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "        self.norm_2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        node_embeddings = self.attention(node_embeddings, mask, graph) + node_embeddings\n",
    "        node_embeddings = self.norm_1(node_embeddings)\n",
    "        node_embeddings = self.feed_forward(node_embeddings) + node_embeddings\n",
    "        node_embeddings = self.norm_2(node_embeddings)\n",
    "        node_embeddings = torch.masked_fill(node_embeddings, ~mask.unsqueeze(-1), 0.0)\n",
    "        return node_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867fc56c03dce698",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Expressiveness of a Transformer as GNN\n",
    "Let's play for a while with the expressiveness of a TransformerLayer in a GNN. For that purpose, let us simplify the graph featurizer used to transform a SMILES to a graph as much as possible, so that the featurized graph contains only the information about the atom tyoe."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e4eb54dd80f7f1"
  },
  {
   "cell_type": "code",
   "source": [
    "from dgllife.utils import BaseAtomFeaturizer, ConcatFeaturizer, atom_type_one_hot\n",
    "\n",
    "atom_type_featurizer = BaseAtomFeaturizer(\n",
    "    featurizer_funcs={\n",
    "        \"h\": ConcatFeaturizer([atom_type_one_hot]),\n",
    "    }\n",
    ")\n",
    "smiles_to_graph_simple = SMILESToBigraph(\n",
    "    node_featurizer=atom_type_featurizer,\n",
    "    add_self_loop=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38de5b1c72850c4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us also define some GNN constructor wrappers for convenience:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db9a6076f9a98a"
  },
  {
   "cell_type": "code",
   "source": [
    "def transformer_gnn(node_features_size: int, n_layers: int) -> GNN:\n",
    "    return GNN(\n",
    "        node_features_size=node_features_size,\n",
    "        hidden_size=64,\n",
    "        output_size=1,\n",
    "        gnn_layer_cls=TransformerLayer,\n",
    "        gnn_layer_kwargs={},\n",
    "        gnn_n_layers=n_layers,\n",
    "        gnn_layer_requires_dense=True,\n",
    "        readout_cls=SumReadout,\n",
    "    )\n",
    "\n",
    "\n",
    "def gat_gnn(node_features_size: int, n_layers: int) -> GNN:\n",
    "    return GNN(\n",
    "        node_features_size=node_features_size,\n",
    "        hidden_size=64,\n",
    "        output_size=1,\n",
    "        gnn_layer_cls=GATLayer,\n",
    "        gnn_layer_kwargs={},\n",
    "        gnn_n_layers=n_layers,\n",
    "        gnn_layer_requires_dense=False,\n",
    "        readout_cls=SumReadout,\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e26eba5e28ca0243",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And define two molecules that are pretty dissimilar:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda57840c03d7afd"
  },
  {
   "cell_type": "code",
   "source": [
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "smiles_a = \"NSCCN\"\n",
    "MolFromSmiles(smiles_a)"
   ],
   "metadata": {
    "collapsed": false,
   },
   "id": "6fd3d0946ae2c05",
   "outputs": [
    {
     "data": {
      "text/plain": "<rdkit.Chem.rdchem.Mol at 0x280143840>",
      "image/png": "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\n"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "smiles_b = \"NS(CC)N\"\n",
    "MolFromSmiles(smiles_b)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df34e6eb7b9b042c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "graph_a = smiles_to_graph_simple(smiles_a)\n",
    "graph_b = smiles_to_graph_simple(smiles_b)\n",
    "batch = dgl.batch([graph_a, graph_b])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc86171486e10508",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 5. Expressiveness of a Transformer as a GNN (2 points).\n",
    "Using the code provided below, run few experiments and answer the following questions:\n",
    "1. Explain why the Transformer-based GNN can (or cannot) distinguish between the two molecules.\n",
    "2. Why can (or cannot) we make the Transformer-based GNN to distinguish between the molecules by manipulating the number of layers?\n",
    "3. Explain why the GAT-based GNN can (or cannot) distinguish between the two molecules.\n",
    "4. Why can (or cannot) we make the GAT-based GNN to distinguish between the molecules by manipulating the number of layers?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c00d11652b85d1fb"
  },
  {
   "cell_type": "code",
   "source": [
    "transformer = transformer_gnn(node_features_size=atom_type_featurizer.feat_size(), n_layers=30)\n",
    "transformer(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fae8178804eb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "gat = gat_gnn(node_features_size=atom_type_featurizer.feat_size(), n_layers=30)\n",
    "gat(batch)"
   ],
   "metadata": {
    "id": "a13dd9ec9efb90b4"
   },
   "id": "a13dd9ec9efb90b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional Encodings\n",
    "As you finished the previous task, you know that our Transformer-based GNN cannot distinguish between the two molecules, and you know the exact reason. To overcome that reason, we can use structural encodings which are the analog of positional encodings used in the original textual Transformer. The purpose of the structural encoding is to encode the structure of a graph into the initial node features. For example, we can use the random walk positional encoding from dgllife package: [RandomWalkPE](https://docs.dgl.ai/en/1.1.x/generated/dgl.transforms.RandomWalkPE.html):\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5c82f3260a0c2850"
   },
   "id": "5c82f3260a0c2850"
  },
  {
   "cell_type": "code",
   "source": [
    "from dgllife.utils.mol_to_graph import construct_bigraph_from_mol\n",
    "from dgl import random_walk_pe\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "class PositionalEncodingFeaturizerBase:\n",
    "    @abstractmethod\n",
    "    def feat_size(self) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class JointFeaturizer:\n",
    "    def __init__(self, atom_featurizer: BaseAtomFeaturizer, pe_featurizer: PositionalEncodingFeaturizerBase):\n",
    "        self.atom_featurizer = atom_featurizer\n",
    "        self.pe_featurizer = pe_featurizer\n",
    "\n",
    "    def feat_size(self) -> int:\n",
    "        return self.atom_featurizer.feat_size() + self.pe_featurizer.feat_size()\n",
    "\n",
    "    def __call__(self, mol: Chem.Mol):\n",
    "        atom_features = self.atom_featurizer(mol)['h']\n",
    "        pe_features = self.pe_featurizer(mol)\n",
    "        return {'h': torch.cat([atom_features, pe_features], dim=-1)}\n",
    "\n",
    "\n",
    "class RandomWalkPEFeaturizer(PositionalEncodingFeaturizerBase):\n",
    "    def __init__(self, n_steps: int = 16):\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def feat_size(self) -> int:\n",
    "        return self.n_steps\n",
    "\n",
    "    def __call__(self, mol: Chem.Mol):\n",
    "        graph = construct_bigraph_from_mol(mol)\n",
    "        return random_walk_pe(\n",
    "            graph, k=self.n_steps\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddceef663c774550",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6. Random walk. (1 points).\n",
    "Given the following code:\n",
    "```\n",
    "smiles = 'C1CCC2CCCCC2C1'\n",
    "mol = MolFromSmiles(smiles)\n",
    "output = RandomWalkPEFeaturizer(n_steps=16)(mol)\n",
    "```\n",
    "Briefly explain the meaning of the `output[i, k]` value. Is it a probability of some event? What is the event?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "524c9eca4085690b"
  },
  {
   "cell_type": "code",
   "source": [
    "node_pe_featurizer = JointFeaturizer(\n",
    "    atom_featurizer=atom_type_featurizer,\n",
    "    pe_featurizer=RandomWalkPEFeaturizer(n_steps=16),\n",
    ")\n",
    "smiles_to_graph_pe = SMILESToBigraph(\n",
    "    node_featurizer=node_pe_featurizer,\n",
    "    add_self_loop=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c90e737702c5c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now observe the effect of the positional encodings on the Transformer-based GNN:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1159b480bf8318a"
  },
  {
   "cell_type": "code",
   "source": [
    "graph_a = smiles_to_graph_pe(smiles_a)\n",
    "graph_b = smiles_to_graph_pe(smiles_b)\n",
    "batch = dgl.batch([graph_a, graph_b])\n",
    "transformer = transformer_gnn(node_features_size=node_pe_featurizer.feat_size(), n_layers=3)\n",
    "transformer(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3d7cc36db923c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Walk and expressiveness of MPNNs\n",
    "Using random walk as a positional encoding can also help MPNN to be more expressive (distinguish more non-isomorphic graphs)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "325168c4a48f62e8"
   },
   "id": "325168c4a48f62e8"
  },
  {
   "cell_type": "code",
   "source": [
    "smiles_a = 'C1CCC2CCCCC2C1'\n",
    "MolFromSmiles(smiles_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b495d6fbe406181d",
   "outputs": [
    {
     "data": {
      "text/plain": "<rdkit.Chem.rdchem.Mol at 0x282fa8820>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAACWCAIAAADCEh9HAAAABmJLR0QA/wD/AP+gvaeTAAAbLklEQVR4nO3daVhTZ9oH8CcLBkRZBEQFV4xaWttRrK1bx16jtQoqLQMyYIqig9sYrXVJlSGI1jLW0gBapWPVVEAaBr0GpWpxx8uOAy69hLGyCqjIgCKSsBiSvB+ezpEXKSokuU9y7t8HP6Amf83Jn5Cc+z48g8FAEEIIdRUfOgBCCFk2rFGEEOoWrFGEEOoWrFGEEOoWrFEWUavVzc3N0CkQ2zU3N6vVaugU6CmsUXh1dXWbNm3y8vJycHBwdnaWyWTQiRB7yWQyJycnBwcHT0/PVatW1dTUQCdChIcnPEFRq9VZWVkZGRkZGRl6vZ75Oo/Hu3jx4sSJEwGzIXa6dOnS5MmT2z5n+Xx+QEBAQECAr69vr169ALNxGdaouT169Cg7O/vo0aOHDx/WaDT0iyKR6O2335bJZOHh4VVVVQ4ODtXV1ba2trBREatotdq+ffs+evSob9++GRkZcXFxp0+ffvz4Mf1dW1vbadOmzZ49+4MPPnBzc4ONyjkGZBYPHjxQKpV+fn49evSg//N8Pn/SpEmxsbGFhYXMHyspKREIBISQuXPnAqZFLDRnzhxCiEAgKCoqYr5YXl6elJTk5+cnFArpcSUQCCZNmqRQKKqqqgDTcgrWqGnV1NTQ9rSxsWl3lN+9e7fDv5KQkMDn8/l8/unTp82cFrHWhQsXBAIBj8eLi4vr8A/U1tb+1pF2584dM6flGqxRk6ioqFAoFNOmTXv2NcL9+/ef+9ejoqIIIR4eHrW1tWZIi1ju4cOHgwYNIoTI5fIX+cO0T0UiEfNzj4+Pj1wub/syFhkR1qgxlZWVKRSKSZMm8Xg8egTb2tr6+fkplcpHjx69+O3odLqpU6cSQnx9ffV6vekCI4vg7+9PCJk8ebJWq33xv6XRaDIzMyUSSduPnry9veVy+c2bN02XloOwRo2gpKSkXXva2dnR9nz8+HHXbrOysrJPnz6EkF27dhk3LbIsO3fuJIQ4OTmVlZV17RYaGxtpnzo4OLTr07y8PKOG5Sis0a7Lz8+Xy+Xe3t7Moenk5BQYGKhUKtVqdfdvPyMjg76evX79evdvDVmi/Px8Ozs7QsihQ4e6f2tNTU2ZmZkRERFtP8ofNmyYVCrNycnBn3u6DGv0pdH2HDlyJHMg9unTRyKRZGZmtrS0GPe+IiIi6AsHjUZj3FtG7NfU1PT6668TQpYsWWLcW25tbc3JyZFKpf369WMO48GDB2Ofdg3W6AvR6XQ5OTkbNmzw8vJiDjtXV1fank+ePDHR/TJPpGXLlpnoLhBrLV261NTfROmBLZVKPTw8mAPb09MzIiIiMzPzpd6K5TKs0c4w37T79+/PHGQDBw6USqXZ2dnmOciYH+vS0tLMcHeIJQ4fPkwIEYlE5nlLR6fT5eXlyeVysVjMHOouLi4m+jHLymCNdoBpT3d3d+aQGjJkCNSPPAkJCfSN19u3b5v5rhGIyspKFxcXQkhiYqL5752+bTVq1Cjm4Hd2dqZ92tzcbP487Ic1+hR9A14ikTg6Oj77BjxgML1eP3fuXELIlClTWltbAZMgM9DpdO+++y4hZNasWbBvU9I+9fHxYZ4OPXv2pKegNDQ0AAZjG6zRp6eD9O7du93pIAUFBdDpfsWcgB0dHQ2dBZmWXC4nhHh4eNTU1EBn+VVpaelvndJXX18PnQ4ed2u0rq5OpVJJJBJ7e/t27Xnr1i3odB04f/68QCDg8/lnzpyBzoJMJScnhz7Kp06dgs7Sgdu3b3c4YJKUlFRdXQ2dDgznarSTFSHFxcXQ6Z5j06ZN9INUHBK1SnV1dYMHDyaEREZGQmd5jsrKSlyJwuBKjXayIuTevXvQ6V6UVqule0g//PBD6CzI+ObNm0cIGT9+vOlOoTM6ZiVKu9clnFqJYuU1Wl5e3p0VISxUUVFBh0T37NkDnQUZ0+7duwkhjo6OpaWl0Fm6opOVKG1XQVol66xRY60IYaf09HT6L/r555+hsyDjKCgo6NmzJyEkNTUVOkt3cXAlilXVKLMi5NnzM7q8IoSdFi1aRAh59dVXGxsbobOg7mpqanrjjTcIIYsXL4bOYkzcWYliDTVKz2575ZVXmIfKyclJIpGoVCqjrAhhIbVaTf+9K1asgM6Cumv58uWEELFYbGXf7BnNzc3Z2dlSqbRv377Mk3To0KFWM8JvwTVK23PEiBHMA2O6FSEsdOPGDTokeuTIEegsqOuOHTvG4/FEItHVq1ehs5hc5ytRdDoddMAusrAaZTYpeHp6Mg+DGVaEsJNCoSCEODs7l5eXQ2dBXXHnzh069BkfHw+dxaw6XIni5uZGn8gWtxLFMq4M2tLSsnfv3sLCwvT09KqqKvrFQYMG+fv7BwYGTpw4kc/nwyYEYTAY/P39MzMzp0yZcvbsWXotPGQp9Hr99OnTz5w5M3PmzKysLObjUK4pKChIT09PTU0tKiqiX3FxcZk1a5a3t7dUKqWfvLEddI93prGxMSoqSiwWtz3CxGKxTCbLzc2FTscKNTU1AwYMIITExMRAZ0EvZ/PmzYQQd3d3Cz33zuhyc3NlMlnbFVM8Hk8sFkdFRbH8o1RW12h4eDjzHyoSiUJCQmBXhLDTuXPnBAKBUCi8ePEidBb0oi5fvmxjY8Pn87Ozs6GzsE5+fn5ERETb16Hh4eHQoTrD6hql5/GOHTv2+PHj0FlY7dNPPyWEDBw48MGDB9BZ0PPV1dUNGTKEELJx40boLKx2/PjxsWPH0ldR0Fk6YwE1ip9EP5dWq50wYQIhJCAgADoLer7g4GBCyJtvvsm1D0W74MiRI+yvUS5+MmN9hEJhWlqas7NzRkbG3//+d+g4qDPffPNNWlqao6Pj999/z2x4QBYNa9RKDBo0KCkpiRAilUpv3LgBHQd17D//+c/HH39MCPn666+HDh0KHQcZB6trdNiwYcyv6LkCAwMXLlzY3NwcEhLS1NQEHQe119LSEhoa2tjYGB4eHhISAh3HMlhECbC6RunZoNw8J7RrEhMTR40alZ+fv2HDBugsqL21a9dev359+PDhdG4CvQiLKAFWh0Mvy97eXqVS2dra7ty585///Cd0HPTUDz/8sGvXLpFIpFKp2l6uBlkBrFFrM3r06G3bthkMhoULF1ZUVEDHQYQQcvfu3bCwMIPBEBsbO2bMGOg4yMiwRq3Q6tWrZ8+eXVdXJ5FIdDoddByu0+v1YWFhtbW177///qpVq6DjIOPDGrVCPB7v22+/7d+//4ULF2JjY6HjcN22bdtOnz7t7u6+f/9+zg7OWzesUevk5uaWmpoqEAiio6MvXboEHYe7/v3vf8fExPD5/IMHD7bdDoesCdao1Zo6deratWtbW1uDg4MfPnwIHYeL6uvrg4ODtVrt+vXrp0+fDh0HmQrWqDXbunXrhAkTKisrlyxZAp2Fi5YtW1ZWVjZu3Di6zAlZK6xRayYUCpOTkx0cHP7xj398++230HG4Ze/evYcOHerVq1dKSgpz8WFklbBGrdywYcPolP2qVatu3rwJHYcrioqK1qxZQwjZs2dP2+vcIKuENWr9goKCwsLCNBpNUFAQDomaQUtLy7x58xoaGhYsWBAaGgodB5kc1ign7Nq1a+TIkfn5+XQzKTKp9evXX7t2bfjw4QkJCdBZkDlgjXKCvb19ampqjx49EhISMjMzoeNYs+PHjycmJtrY2CQnJ+PQJ0dgjXLF2LFj6ZDookWL7t27Bx3HOlVXVy9cuJAOfb711lvQcZCZYI1yyJo1a/z8/Gpra0NCQnBI1Oj0ev38+fOrq6tnzJhBl4oijsAa5RAej7dv377+/fufP39++/bt0HGsTWxs7KlTp/r27XvgwAEc+uQUrFFucXNzO3DgAJ/Pj4qK+umnn6DjWI/c3Nzo6Gj6jQqHPrkGa5Rz3nvvvU8++aS1tTU0NLS+vh46jjVQq9WhoaFarXbdunW+vr7QcZC5YY1y0Wefffb222+XlZX9+c9/hs5iDZYsWVJUVOTj47NlyxboLAgA1igX0dNxHBwc0tPT9+/fDx3Hsu3bty81NbVXr170lDLoOAgA1ihHeXl50SHRlStX/vLLL9BxLFVxcfHq1asJIbt378ahT87CGuWuoKAgiURCh0Sbm5uh41ielpaWoKCghoaGjz76aP78+dBxEBisUU77+uuvR44ceePGjY0bN0JnsTwymezatWteXl6JiYnQWRAkrFFOY9a4KRSKo0ePQsexJCdOnIiPj7exsUlJSXFwcICOgyBhjXKdj4/P1q1bcUj0pTBDn9u2bcOhT4Q1isjatWt9fX1rampCQ0NxSPS59Hq9RCK5f//+e++9R5eKIo7DGkWEmb05d+7cjh07oOOw3fbt27Ozs+nQJ5+PzyCENYoIIYQwk+CRkZE4JNqJvLw8uVzOXMIaOg5iBaxR9KsZM2asWbOmtbV1/vz5jx8/ho7DRnTo88mTJ5988omfnx90HMQWWKPoqc8///ytt94qLS3FIdEOLV26tLCw0MfH57PPPoPOglgEaxQ9xZy+o1KplEoldBx2OXDgQEpKir29PV7pE7WDNYr+Hy8vL3oFoRUrVty6dQs6DlsUFxdLpVLyv4EF6DiIXbBGUXthYWHz58/XaDQhISFPnjyBjgNPq9XOnz+/oaEhKCjoo48+go6DWAdrFHWALtq4evUqDokSQmQy2eXLl4cNG0aXuSDUDtYo6gAzJBoXF3fs2DHoOJBOnjz51VdfCYVCHPpEvwVrFHVs3LhxMTExBoMhPDy8qqoKOg6M//73vwsWLDAYDHTRNXQcxFJC6ACIvdatW/fDDz/k5OQMGDAAOgsYHo/3zjvvrF27FjoIYi98NYoQQt2CNYp+0xdffHHhwgVXV9d79+4ZOKm6utrd3f3ChQu4agB1AmsUdSwvLy8qKoq5tD10HBjMqoFNmzb961//go6DWAprFHWAGR5fs2YNx4fHZ8yY8fHHH9PrUeOqAdQhrFHUgWXLlhUWFo4dO3bbtm3QWeDFxsbiqgHUCaxR1J5SqUxOTra3t8crBlP0etS9e/dWqVTfffcddBzEOlij6P8pKSmhw+O7du3C4XHG8OHD6aqB5cuX46oB1A7WKHpKq9XSdwCDgoLCwsKg47DLggULQkNDNRoNfdcYOg5iEaxR9NSnn36Kw+Od2LNnz4gRI65cubJp0yboLIhFsEbRr06ePBkXFycUCpOTk3F4vEPMqoEvv/yS46sGUFtYo4iQNsPjW7dunTBhAnQc9ho3btzmzZsNBsOiRYs4u2oAtYM1iojBYAgPD79///7UqVNxePy51q9fP336dPqNR6/XQ8dB8LBGEdmxY0dWVpabm1tKSopAIICOw3Z8Pv/gwYP9+vX78ccf4+LioOMgeFijXHflypXIyEh6xWAub3J6Ke7u7vv37+fxeBs3brx8+TJ0HAQMa5TTmKHP1atXz549GzqOJXn//fdXrVrFnCIGHQdBwhrlNHoy+ejRo3HoswtiY2PHjBlTUlKycuVK6CwIEtYod6lUqoMHD9rb26tUKltbW+g4lkckEqlUqt69e3/33XfJycnQcRAYrFGOKikpoYs2EhMTR40aBR3HUg0fPlyhUJD/LXOBjoNgYI1yEb1i8OPHjwMDAxcuXAgdx7KFh4eHhISo1Wq8HjVnYY1yEV1CPHToUBz6NIqkpCSxWHzlypW//vWv0FkQAKxRzvnxxx+//PJLesVgR0dH6DjWgA6J2tjYfPHFF1lZWdBxkLlhjXJLTU0Nnb2JiYnBoU8jevPNN6Ojo5l5MOg4yKywRjnE8L+Lzv/+979fv349dBxrI5PJpk2bxmwngI6DzAdrlEPi4uKOHTvm6uqampqKQ59Gx+fzk5OT3d3dT548+dVXX0HHQeaDNcoVV69e3bhxIw59mhQzJCqTyXBIlDuwRjlBo9HQ03GkUumcOXOg41izmTNnrly5kp5S1tDQAB0HmQPWKCesWLHi1q1br7322ueffw6dxfpt3759zJgxxcXF9KpWyOphjVo/lUqlVCrp0KednR10HOsnEom+//773r17HzhwICUlBToOMjmsUSvHXF09Pj7+lVdegY7DFWKxmK4iXbp0KQ6JWj2sUWvW2tpKhz7/+Mc/Llq0CDoOtyxevPhPf/oTs4oQOg4yIaxRaxYZGfnTTz8NHDgwKSkJOgsX7d69e+jQoXl5eXK5HDoLMiGsUat17ty5HTt2CIXCtLS0Pn36QMfhIkdHx7S0NBsbm+3bt2dnZ0PHQaaCNWqdampqQkJCdDpddHT0xIkToeNw1/jx46OiovR6vUQiwSFRa4U1aoWYy/++8847MpkMOg7Xbdy48Q9/+EN1dfXChQtxSNQqYY1aIYVCcfToUWdn54MHD+LQJzg+n69UKl1dXU+cOBEfHw8dBxkf1qi1uXHjBh363L9//6BBg6DjIEII8fDwUCqVdEj02rVr0HGQkWGNWhWNRhMUFNTc3PyXv/xl7ty50HHQU7NmzVqxYkVLS0tQUBAOiVoZVteoXq9nfkUvYuXKlb/88strr732t7/9DToLam/Hjh2/+93viouLV69eDZ3FYlhECbC6RktLS5lf0XOlp6fv37/f1tY2NTUVhz5ZSCQSpaSk9OzZc9++fampqdBxLINFlACraxS9uIqKiiVLlhBCEhISRo8eDR0Hdczb25uuIl2+fHlZWRl0HGQcWKPWoLW1NTg4uK6uLiAggE7QI9aKiIgIDg6ur6+fN2+eVquFjoOMwAJqdMuWLSdOnIBOwWpRUVF06PObb76BzoKeb/fu3UOGDMnNzY2OjobOwmonTpzYsmULdIoXYGCx8PBwJqdIJAoJCcnJyYEOxTrnzp0TCARCofDixYvQWdCLunz5so2NDZ/Pz87Ohs7COvn5+RERET179mSe/nRygbVYXaONjY1RUVFisZjH4zH/oWKxWCaT5ebmQqdjhZqaGnpFkJiYGOgs6OVs3ryZEOLu7n7//n3oLKyQm5srk8nEYjHzZOfxeGKxOCoqqrGxETpdZ3gGS5hOa2lp2bt3b2FhYXp6elVVFf3ioEGD/P39AwMDJ06cyOdbwLsTRmcwGPz9/TMzM6dMmXL27FkcWLIser1++vTpZ86cmTlzZlZWVtvXCpxSUFCQnp6emppaVFREv+Li4jJr1ixvb2+pVNr2NSl7Qff4y9HpdDk5OVKp1NPTk/knuLq6SiSSzMzMJ0+eQAc0K4VCQQhxdnYuLy+HzoK64s6dOy4uLoSQ+Ph46CxmxTyRPTw8mCeym5sbfSJrtVrogC/Hwmq0rfz8fLlcPmLECOZh6NOnD30YWlpaoNOZ3I0bN+jJoUeOHIHOgrru2LFjPB5PJBJdvXoVOovJtba20vbs168f87QdPHiwVCrNycnR6XTQAbvIgmuUQfu07RUynJycJBKJSqVSq9XQ6UxCrVbTf++KFSugs6DuWr58OSFELBY/fvwYOotJNDc3Z2dnS6XSvn37Mk/SoUOH0vbU6/XQAbvLGmqUUVJSolAoJk2axDxUPXv29PPzUyqVVnaA0iuCvPrqqyx/6x29iKampjfeeIMQsnjxYugsxtTY2JiZmSmRSBwcHJinpLe3t1wuz8vLg05nTFZVo4yysjLap8zb9ra2trRPHz16BJ2uu9LT0+m/6Oeff4bOgoyjoKCAfpaSmpoKnaW7NBoNbc9evXq1a8+bN29CpzMJ66xRRnl5uUKhmDZtmlAopA+nQCCYNGmSQqGw0LNMKioq6BVB9uzZA50FGdPu3bsJIY6OjqWlpdBZuuLhw4dKpdLPz08kEtHnGp/P9/HxkcvlhYWF0OlMy8prlFFTU0MfYxsbm3Z9eu/ePeh0L0qr1dIrgnz44YfQWZDxzZs3jxAyfvx4CzrnpLa2lj6zevTowbQnfWbduXMHOp2ZcKVGGQ8ePOjwUY+NjS0uLoZO9xybNm0ihHh6etbW1kJnQcZXV1c3ePBgQkhkZCR0lueorKxMSkry8/N79ue8qqoq6HTmxrkaZdTV1alUKolEYm9v3+4dnFu3bkGn68D58+cFAgGfzz9z5gx0FmQqOTk59FE+deoUdJYO3L59u8NPHZKSkqqrq6HTgeFujTKYzxN79+7drk8LCgqg0/3q4cOH9Iog0dHR0FmQadGL2nt4eNTU1EBn+VVpaWm79rSzs6Of2dbX10Ong4c1+lRTUxPtU0dHR6ZPhw0bRs9uAwym1+vpFUGmTJnS2toKmASZgU6ne/fddwkhs2bNgj2nkp6R7ePj8+wZhA0NDYDB2AZrtAPMrIW7uztzAA0ZMgTqbOGEhAQ6U3D79m0z3zUCUVlZSYdEExMTzX/vtD1HjRrFHPzOzs50PrC5udn8edgPa7QzTJ/279+fOaQGDhwolUqzs7PNM/mbn59Phz7T0tLMcHeIJQ4fPkwIEYlE169fN8Pd6XS6vLw8uVzedsGSi4sLd6aruwNr9IXQTQobNmzw8vJiDjIzrERpamp6/fXXCSHLli0z0V0g1lq6dCl9m16j0ZjoLjpcEeLp6RkREWGJK0KgYI2+NPojz8iRI5nDznQrUSIiIkz9REKsxXwTXbJkiXFvufMVIVYw5G5mWKNdR/vU29ubORCdnJwCAwOVSqVRVqJkZGTQE0rM82MdYiHmLZ1Dhw51/9boh6gRERFubm7PfoiK7dllWKNGwKxEefZ0kC6vRKmsrKRDn7t27TJuWmRZdu7cSb9Dl5WVde0WuLMiBArWqDEZayWKTqebOnUqIcTX1xdfIyB/f39CyOTJk1/qzUoOrgiBgjVqEhUVFd1ZiRIVFUUI8fDwwKFPZGgzfCGXy1/kD//WipCioiLTh+UirFHT6mQlyt27dzv8KwkJCXw+n8/nnz592sxpEWtduHBBIBDweLy4uLgO/wCzIuTZI407K0KgYI2aSScrUdquESspKaFXpps7dy5gWsRCc+bMoeXY9kVleXk5rggBZxlXBrUmjx49ys7OPnr06OHDhzUaDf2inZ3d5MmT161bFxYWVlVV5eDgUF1dbWtrCxsVsUpLS4u7u3t9fX2/fv3S0tLi4uLOnj3b0NBAf9fW1nbatGmzZ8/+4IMP2n4Qj8wAaxSMWq3OysrKyMjIyMjQ6/XM13k83sWLF+leUYTaunTp0uTJk9s+Z/l8fkBAQEBAgK+vb9uPkpA5YY3Cq6ur27FjR1paWllZmUgkWrVqVWxsLHQoxFIymSw+Pr6lpWXAgAEBAQGRkZH42hMc1iiLqNVqoVCIP8ujzjU3N7e2tuJrT/bAGkUIoW7hQwdACCHLhjWKEELdgjWKEELdgjWKEELd8n8oIw4hsi8bZgAAALR6VFh0cmRraXRQS0wgcmRraXQgMjAyMi4wOS41AAB4nHu/b+09BiDgZYAAJiDmAmJuIG5gZGNIAIkxYaeZGfHzMWluBkYGRiYGJmYGZhYGZlYGVjYGNnYGdg4GDk4GFgYGTlYGEZADWIGKmFlY2Vg5OdjFm4ACjFDXMXBN3uXswMDgsB/EgbLtQexLty7YnwuZbQ9RBhJbsBRJfD9UHEg3wMT3w8SB5hxAMvMAkpn7YWaKAQA+AC12zy/rMAAAAQJ6VFh0TU9MIHJka2l0IDIwMjIuMDkuNQAAeJyNkl0OgjAMx993il4A0m5usEf5iDEGSBS9g+/eP7Ya6IhmYaNJO377b21nQMa1uzxfsA7bGQOAmS/GCA+HiGYAcaDpT+cR2vnYLCvtdB/nGxACEe/huWWP8zQsKwQtuBJ9bYODAsvKszTvKBG/zgLaFMxwjjkqg6NQ1exYct76P9yBuVWm0P8/oE8Fi4xiYLDYJVkJuUuzFnJXfeKGzIDclvT0zOH92G169e1eM42ddk+m1R5xAE5bQWwHrTixea2rhEGrR2yVlojYaq0DsUVNlsTSnEgE1/hzMVR1SSW9uMTLk2XfvAE2SZE+ML4l9wAAAGx6VFh0U01JTEVTIHJka2l0IDIwMjIuMDkuNQAAeJxljEEKwCAMBL/So0ISTEKs4HG/5eNbWhRaL8vuMgwUQILdCcvQYySXEs0qcZEzqM/5LpXqWhupmHo49ecu1HmVifBi+Kfkr5M36abI4wJv2SH7gkk8+QAAAABJRU5ErkJggg==\n"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "smiles_b = 'C1CCC(C1)C2CCCC2'\n",
    "MolFromSmiles(smiles_b)"
   ],
   "metadata": {
    "id": "7a40bd3dea5767d"
   },
   "id": "7a40bd3dea5767d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 7. Expressiveness of a MPNN (2 points).\n",
    "Using the code provided below, run few experiments and answer the following questions:\n",
    "1. Explain why the GAT-based GNN can (or cannot) distinguish between the two molecules.\n",
    "2. Explain why the Transformer-based GNN with random walk positional encodings can (or cannot) distinguish between the two molecules.\n",
    "3. Explain why the GAT-based GNN with random walk positional encodings can (or cannot) distinguish between the two molecules."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2308b8a27ebcd713"
  },
  {
   "cell_type": "code",
   "source": [
    "graph_a = smiles_to_graph_simple(smiles_a)\n",
    "graph_b = smiles_to_graph_simple(smiles_b)\n",
    "batch = dgl.batch([graph_a, graph_b])\n",
    "gat = gat_gnn(node_features_size=atom_type_featurizer.feat_size(), n_layers=3)\n",
    "gat(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0f9bad16b6026e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "graph_a = smiles_to_graph_pe(smiles_a)\n",
    "graph_b = smiles_to_graph_pe(smiles_b)\n",
    "batch = dgl.batch([graph_a, graph_b])\n",
    "transformer = transformer_gnn(node_features_size=node_pe_featurizer.feat_size(), n_layers=3)\n",
    "transformer(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0e56cf32385de74",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "graph_a = smiles_to_graph_pe(smiles_a)\n",
    "graph_b = smiles_to_graph_pe(smiles_b)\n",
    "batch = dgl.batch([graph_a, graph_b])\n",
    "gat = gat_gnn(node_features_size=node_pe_featurizer.feat_size(), n_layers=3)\n",
    "gat(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7afa6857d6a27b8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments\n",
    "Ok, now we can quickly play with the Transformer-based and GAT-based GNNs and see how it performs on the molecular property prediction task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f73a37520e03b08"
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from torchmetrics import MeanAbsoluteError as MAE\n",
    "from torchmetrics import MeanSquaredError as MSE\n",
    "from torchmetrics import PearsonCorrCoef as PCC\n",
    "\n",
    "metrics = {\n",
    "    \"mae\": MAE(),\n",
    "    \"mse\": MSE(),\n",
    "    \"pcc\": PCC(),\n",
    "}\n",
    "\n",
    "\n",
    "def get_time_stamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc8e67041a8ce0ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 8. Train GAT (2 points).\n",
    "1. Tune hyperparameters of a GNN with `GAT` as a layer to obtain at most 2.0 MAE on the validation set. You can modify the GNN/MPNN architecture so it uses regularization tricks. You can also change the featurization functions. Don't change the validation batch size. If your validation MAE is in (2.0, 2.5], you can obtain 1 point.\n",
    "2. Report the obtained MAE on the validation and test set (only the former need to be lower than 2.0 MAE).\n",
    "3. Provide the link to the final run: [your link]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150ba1690f21ae1"
  },
  {
   "cell_type": "code",
   "source": [
    "node_featurizer = CanonicalAtomFeaturizer()\n",
    "dataset = FreeSolv(\n",
    "    smiles_to_graph=SMILESToBigraph(\n",
    "        node_featurizer=node_featurizer,\n",
    "        add_self_loop=True,\n",
    "    )\n",
    ")\n",
    "splitter = ScaffoldSplitter()\n",
    "train, valid, test = splitter.train_val_test_split(dataset)  # it's deterministic\n",
    "\n",
    "model = GNN(\n",
    "    node_features_size=node_featurizer.feat_size(),\n",
    "    hidden_size=64,\n",
    "    output_size=1,\n",
    "    gnn_layer_cls=GATLayer,\n",
    "    gnn_layer_kwargs={},\n",
    "    gnn_n_layers=4,\n",
    "    gnn_layer_requires_dense=False,\n",
    "    readout_cls=SumReadout,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    run_dir=\"experiments\",\n",
    "    train_dataset=train,\n",
    "    valid_dataset=valid,\n",
    "    train_metrics=metrics,\n",
    "    valid_metrics=metrics,\n",
    "    train_batch_size=32,\n",
    "    model=model,\n",
    "    logger=WandbLogger(\n",
    "        logdir=\"runs/mpnn\",\n",
    "        project_name=\"mldd23\",\n",
    "        experiment_name=f\"gat_{get_time_stamp()}\",\n",
    "    ),\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    n_epochs=100,\n",
    "    device=\"cpu\",\n",
    "    valid_every_n_epochs=1,\n",
    ")\n",
    "\n",
    "valid_metrics = trainer.train()\n",
    "test_metrics = trainer.test(test)\n",
    "trainer.close()\n",
    "print(f\"Validation metrics: {valid_metrics}\")\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88d133d593488f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 9. Train Transformer (2 points).\n",
    "1. Tune hyperparameters of a GNN with `Transformer` as a layer to obtain at most 2.0 MAE on the validation set. You can modify the GNN/MPNN architecture so it uses regularization tricks. You can also change the featurization functions. Don't change the validation batch size. If your validation MAE is in (2.0, 2.5], you can obtain 1 point.\n",
    "2. Report the obtained MAE on the validation and test set (only the former need to be lower than 2.0 MAE).\n",
    "3. Provide the link to the final run: [your link]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8daf4b31a12b2d44"
  },
  {
   "cell_type": "code",
   "source": [
    "node_featurizer = CanonicalAtomFeaturizer()\n",
    "dataset = FreeSolv(\n",
    "    smiles_to_graph=SMILESToBigraph(\n",
    "        node_featurizer=node_featurizer,\n",
    "        add_self_loop=True,\n",
    "    )\n",
    ")\n",
    "splitter = ScaffoldSplitter()\n",
    "train, valid, test = splitter.train_val_test_split(dataset)  # it's deterministic\n",
    "\n",
    "model = GNN(\n",
    "    node_features_size=node_featurizer.feat_size(),\n",
    "    hidden_size=64,\n",
    "    output_size=1,\n",
    "    gnn_layer_cls=TransformerLayer,\n",
    "    gnn_layer_kwargs={},\n",
    "    gnn_n_layers=4,\n",
    "    gnn_layer_requires_dense=True,\n",
    "    readout_cls=SumReadout,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    run_dir=\"experiments\",\n",
    "    train_dataset=train,\n",
    "    valid_dataset=valid,\n",
    "    train_metrics=metrics,\n",
    "    valid_metrics=metrics,\n",
    "    train_batch_size=32,\n",
    "    model=model,\n",
    "    logger=WandbLogger(\n",
    "        logdir=\"runs/mpnn\",\n",
    "        project_name=\"mldd23\",\n",
    "        experiment_name=f\"transformer_{get_time_stamp()}\",\n",
    "    ),\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    n_epochs=100,\n",
    "    device=\"cpu\",\n",
    "    valid_every_n_epochs=1,\n",
    ")\n",
    "\n",
    "valid_metrics = trainer.train()\n",
    "test_metrics = trainer.test(test)\n",
    "trainer.close()\n",
    "print(f\"Validation metrics: {valid_metrics}\")\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48dee744e7245865",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
